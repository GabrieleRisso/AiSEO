# AiSEO - AI Search Engine Optimization Dashboard

Track your brand's visibility in AI-generated search results. AiSEO scrapes Google AI Mode responses, analyzes brand mentions and citation patterns, and provides actionable SEO insights through an interactive dashboard.

## Overview

AiSEO is a full-stack application that:

1. **Scrapes** Google AI Mode responses for e-commerce queries
2. **Analyzes** which brands are mentioned and their positions
3. **Tracks** citation sources used by AI systems
4. **Visualizes** visibility trends over time in a dashboard
5. **Suggests** SEO improvements based on data patterns

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Frontend     â”‚     â”‚     Backend      â”‚     â”‚     Scraper      â”‚
â”‚  React + Vite    â”‚â—„â”€â”€â”€â–ºâ”‚  FastAPI + SQLiteâ”‚â—„â”€â”€â”€â–ºâ”‚  undetected-     â”‚
â”‚  TailwindCSS     â”‚     â”‚  SQLModel ORM    â”‚     â”‚  chromedriver    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â–¼                        â–¼                        â–¼
   Dashboard UI            REST API (12          Google AI Mode
   localhost:5173          endpoints)            (udm=50 param)
                           localhost:8000
```

**Data Flow:**
```
Google AI Mode â†’ Scraper â†’ JSON files â†’ Backend imports â†’ SQLite DB â†’ API â†’ Dashboard
```

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- Google Chrome browser

### 1. Backend Setup

```bash
cd backend

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env

# Start the server
uvicorn main:app --reload --port 8000
```

API available at: http://localhost:8000
API docs at: http://localhost:8000/docs

## ğŸ“š API Documentation

Complete API documentation is available in the `docs/` folder:

- **[API Reference](docs/api/README.md)** - Complete endpoint documentation
- **[Examples](docs/examples/README.md)** - Request/response examples and code samples
- **[Postman Collection](docs/postman/README.md)** - Postman setup and usage guide

**Interactive Documentation:**
- **Backend Swagger UI**: http://localhost:8000/docs
- **Backend ReDoc**: http://localhost:8000/redoc
- **Scraper API Docs**: http://localhost:5000/docs
- **Postman**: http://localhost:8000/docs/postman

### 2. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env

# Start development server
npm run dev
```

Dashboard available at: http://localhost:5173

### 3. Running the Scraper

```bash
# From project root (with venv activated)

# Basic scrape
python scripts/scrape_google_ai.py "best ecommerce platform"

# Headless mode (no browser window)
python scripts/scrape_google_ai.py "best ecommerce platform" --headless

# With debug screenshots
python scripts/scrape_google_ai.py "best ecommerce platform" --screenshot
```

Results saved to: `data/results/google/{query}.json`

## Database Schema

### Entity Relationship Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Brand     â”‚       â”‚ PromptBrandMention  â”‚       â”‚   Prompt    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)     â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ brand_id (FK)       â”‚       â”‚ id (PK)     â”‚
â”‚ name        â”‚       â”‚ prompt_id (FK)      â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ query       â”‚
â”‚ type        â”‚       â”‚ mentioned           â”‚       â”‚ run_number  â”‚
â”‚ color       â”‚       â”‚ position            â”‚       â”‚ response_txtâ”‚
â”‚ variations  â”‚       â”‚ sentiment           â”‚       â”‚ scraped_at  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ context             â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   Source    â”‚       â”‚    PromptSource     â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
â”‚ id (PK)     â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ source_id (FK)      â”‚              â”‚
â”‚ domain      â”‚       â”‚ prompt_id (FK)      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ url (unique)â”‚       â”‚ citation_order      â”‚
â”‚ title       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ description â”‚
â”‚ published_dtâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tables

| Table | Description | Key Fields |
|-------|-------------|------------|
| **Brand** | Tracked brands (1 primary + competitors) | `id`, `name`, `type` (primary/competitor), `color`, `variations` |
| **Prompt** | Scraped query results | `query`, `run_number`, `response_text`, `scraped_at` |
| **PromptBrandMention** | Brand mentions per prompt | `position` (1=first), `sentiment`, `mentioned` (bool), `context` |
| **Source** | Cited websites | `domain`, `url` (unique), `title`, `description`, `published_date` |
| **PromptSource** | Links prompts to sources | `citation_order` |

### Tracked Brands (Default)

| Brand | Type | Color | Purpose |
|-------|------|-------|---------|
| Wix | primary | Cyan | Your brand (visibility focus) |
| Shopify | competitor | Amber | Main competitor |
| WooCommerce | competitor | Purple | Open-source competitor |
| BigCommerce | competitor | Pink | Enterprise competitor |
| Squarespace | competitor | Emerald | Design-focused competitor |

## API Endpoints

See [API Documentation](docs/API.md) for complete endpoint reference.

**Quick Links:**
- [API Reference](docs/api/README.md) - All endpoints
- [Examples](docs/examples/README.md) - Code samples
- [Postman Collection](docs/postman/README.md) - Testing guide

## ğŸ“ Project Structure

```
AiSEO/
â”œâ”€â”€ docs/                         # ğŸ“š All API documentation
â”‚   â”œâ”€â”€ api/                      # API endpoint reference
â”‚   â”œâ”€â”€ examples/                 # Code examples
â”‚   â””â”€â”€ postman/                  # Postman collection
â”œâ”€â”€ backend/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py                   # API endpoints (1200+ lines)
â”‚   â”œâ”€â”€ models.py                 # SQLModel ORM models
â”‚   â”œâ”€â”€ schemas.py                # Pydantic response schemas
â”‚   â”œâ”€â”€ database.py               # DB connection and init
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â”œâ”€â”€ .env.example              # Environment template
â”‚   â”œâ”€â”€ aiseo.db                  # SQLite database
â”‚   â”œâ”€â”€ scripts/                  # Utility scripts (see scripts/README.md)
â”‚   â”‚   â”œâ”€â”€ seed_data.py          # Initial data seeding
â”‚   â”‚   â”œâ”€â”€ sync_brand_mentions.py # Re-sync brand detection
â”‚   â”‚   â””â”€â”€ ...                   # Historical data scripts
â”‚   â””â”€â”€ backups/                  # Database exports
â”‚       â”œâ”€â”€ database_export.json
â”‚       â””â”€â”€ aiseo.db.backup
â”‚
â”œâ”€â”€ frontend/                     # React dashboard
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx               # Main app component
â”‚   â”‚   â”œâ”€â”€ config.ts             # API configuration
â”‚   â”‚   â”œâ”€â”€ api/client.ts         # API client
â”‚   â”‚   â”œâ”€â”€ types/index.ts        # TypeScript interfaces
â”‚   â”‚   â”œâ”€â”€ pages/                # Route pages (6)
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx     # Main metrics dashboard
â”‚   â”‚   â”‚   â”œâ”€â”€ Brands.tsx        # Brand management
â”‚   â”‚   â”‚   â”œâ”€â”€ Prompts.tsx       # Query library
â”‚   â”‚   â”‚   â”œâ”€â”€ Sources.tsx       # Citation analysis
â”‚   â”‚   â”‚   â”œâ”€â”€ Suggestions.tsx   # SEO recommendations
â”‚   â”‚   â”‚   â””â”€â”€ Settings.tsx      # App settings
â”‚   â”‚   â”œâ”€â”€ components/           # Reusable components
â”‚   â”‚   â”‚   â”œâ”€â”€ layout/           # Header, Sidebar, MobileNav
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/               # MetricCard, DataTable, Modal, Toast
â”‚   â”‚   â”‚   â”œâ”€â”€ charts/           # VisibilityChart, BrandComparison
â”‚   â”‚   â”‚   â””â”€â”€ search/           # GlobalSearch, SearchResults
â”‚   â”‚   â””â”€â”€ hooks/                # Custom React hooks (7)
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ src/                          # Scraper library
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â””â”€â”€ google_ai_scraper.py  # Main Google AI scraper
â”‚   â”œâ”€â”€ config/settings.py        # Pydantic settings
â”‚   â””â”€â”€ utils/                    # Logger, exceptions
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ scrape_google_ai.py       # CLI entry point
â”‚
â”œâ”€â”€ data/                         # Runtime data
â”‚   â”œâ”€â”€ results/google/           # Scrape results (JSON)
â”‚   â””â”€â”€ screenshots/              # Debug screenshots
â”‚
â”œâ”€â”€ tests/                        # Test suites (empty)
â”œâ”€â”€ pyproject.toml                # Python project config
â””â”€â”€ .env.example                  # Root environment template
```

## Environment Variables

### Root `.env` (Scraper)

```bash
BROWSER_HEADLESS=false           # Show browser window during scrape
BROWSER_TIMEOUT_SECONDS=60       # Page load timeout
SCRAPER_TAKE_SCREENSHOTS=false   # Capture debug screenshots
LOG_LEVEL=INFO
```

### Backend `backend/.env`

```bash
CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173  # Allowed origins
DATABASE_URL=                    # Optional: PostgreSQL connection string
LOG_LEVEL=INFO
DEBUG=false
```

### Frontend `frontend/.env`

```bash
VITE_API_BASE_URL=http://localhost:8000/api  # Backend API URL
```

## Key Concepts

### Visibility Score

Percentage of tracked queries where your brand is mentioned in the AI response.

```
Visibility = (queries_with_mention / total_queries) Ã— 100
```

Position affects the visibility contribution:
- Position 1 (first mentioned) = 100%
- Position 2 = 80%
- Position 3 = 60%
- And so on...

### Sentiment Analysis

Brand mentions are classified as:
- **Positive**: Contains words like "excellent", "powerful", "recommended", "best"
- **Negative**: Contains words like "limited", "expensive", "complicated", "lacks"
- **Neutral**: Default when no sentiment keywords detected

### Source Classification

Cited sources are automatically categorized:
- **Brand**: Official brand sites (shopify.com, wix.com)
- **Blog**: Contains `/blog/` in URL, Medium, Dev.to
- **Community**: Reddit, Quora, Stack Exchange
- **News**: Forbes, TechCrunch, BusinessInsider
- **Review**: G2, Capterra, Trustpilot
- **Other**: Uncategorized

### Run (Multiple Scrapes)

The same query can be scraped multiple times to track changes:
- `run_number`: 1, 2, 3, etc.
- Allows trend analysis over time
- Dashboard aggregates across runs

## Tech Stack

| Layer | Technologies |
|-------|-------------|
| **Frontend** | React 19, TypeScript, Vite, TailwindCSS 4, Recharts, React Router 7 |
| **Backend** | FastAPI, SQLModel (Pydantic + SQLAlchemy), SQLite, Uvicorn |
| **Scraper** | undetected-chromedriver, Selenium, BeautifulSoup |
| **Styling** | Dark "Data Observatory" theme, CSS variables, Glassmorphism |

## Development

### Adding a New Brand

1. POST to `/api/brands`:
```json
{
  "id": "magento",
  "name": "Magento",
  "type": "competitor",
  "color": "#f97316",
  "variations": ["Magento", "Adobe Commerce", "magento.com"]
}
```

2. System automatically scans all existing prompts for mentions

### Running Utility Scripts

See `backend/scripts/README.md` for data management scripts.

### Building for Production

```bash
# Frontend build
cd frontend
npm run build
# Output: frontend/dist/

# Backend
# Use gunicorn or similar for production
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker
```

## Scraper Details

The Google AI Mode scraper (`src/scrapers/google_ai_scraper.py`):

1. Uses `undetected-chromedriver` to avoid bot detection
2. Navigates to `google.com/search?udm=50&q={query}` (AI Mode)
3. Handles cookie consent dialogs automatically
4. Waits for AI response to generate (up to 60s)
5. Extracts response text (headings, lists, tables)
6. Extracts all source citations with metadata
7. Saves structured JSON to `data/results/google/`

## License

MIT
